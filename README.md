# EPFL Machine Learning Course CS-433 Project 1 
Machine Learning Course, Fall 2016 (a.k.a. PCML)
Kaicheng-Yu's version, Group #2

This README file is for explanation of python code written for the
first project. Note that the structure of folders should be strictly 
following the documentation.

# Use method
Call methods inside implementations.py to get weight, and use to produce 
predictions accordingly.

The following function would produce the best result.
```python
logistic_regression_best(y, tx, lambda_, gamma, max_iters)
```
Note that you should also call 
```python
transformed_data = compose_best_submission_feature(tx)
```
for the testing data matrix, in order to get the correct predictions.

# Structure

## Data
All data file should be put under projects/project1/dataset
It should have the following files to perform the basic operations
* train.csv
* test.csv
The following two are recommended to use for rapid test
* reduced_train.csv
* reduced_test.csv
It could be generated by running the following method in `data_utils.py`
```python
truncate_csv(10000)
```
Make sure that you _set the filename correctly_ before running methods 
 inside `test.py`
### Setting which data to use
```python
# In helpers.py #

# Set to True for quick experiment, False to load whole dataset.
reduced = True
if reduced:
    train_filename = 'reduced_train.csv'
    test_filename = 'reduced_test.csv'
else:
    train_filename = 'train.csv'
    test_filename = 'test.csv'
```
## Source code
All source codes are under folder projects/project1/scripts. And brief 
of functionality will be provided below, according to their importance.

### implementations.py
Python file required.
This file consists of all six method required by the project sheet
In addition, it contains one method that would generate the best result.
Note that, you should give RAW data matrix, i.e. in shape of `(nb_sample, 30)` as 
input to all the methods here.
All the methods except the logistic_regression_best is aiming to produce the baseline
statistics. Thus, standardization is the only data manipulation.
For more information on tests, please refer to `test.py`, we compose a complex experiment 
phases there.

### model.py
Machine learning model engine.
Contains the LogisticRegression and engine for smarter stochastic gradient descent (SGD),
cross-validation. It implements the optimizers including SGD, normal equations and 
cross-validation of given parameters. It also support L1, L2 normalization.
Due to the distribution of work, only LogisticRegression is fully tested for
fitting data, and cross-validation.
LinearRegression model should also work but not fully tested.
The goal of this class is not only specific to this learning project, but also for reusable and scalable
to other problems, models.

### cost.py
Python file for all cost related methods

### gradient.py
Python file for all gradient related method

### data_utils.py
Python file for data manipulation. 
There are 3 categories of these methods, data cleaning, transformation and
data space composition. For detailed information, please refer to the documentation
inside the file.

### helpers.py
Python file for I/O, path manipulation. 
It provide specific helper function to retrieve the absolute path
to datafile, in order to support multi-machine working.

### regularizer.py
Python file for regularizers

### plots.py
Python file for plots. Make sure you could import `matplotlib.pyplot`
on your machine to call methods inside this file.

### learning_model.py
Python files contains the learning models.
The whole idea is to have learning model separated from optimizer
Would be called by LinearRegression model.

Available model listed:
* least_squares
* sigmoid
* least_squares_GD

### test.py
This python file contains every test case based on our logistic
regression model.
The best selected model is put inside the implementations.py
You could skip this class, but to run, make sure the given files are put 
under data_set directory.
You could try a lighter version for quick test with reduced_train.csv and 
reduced_test.csv, which would be provided in our submission.

### test_plot.py
Test file for `plots.py` Routines to generate specific plots, including
- PCA
- Histograms for all data feature
Noted that, it is mainly to separate the `matplotlib.pyplot`,
prevent it being called in test.py.
Make sure your python environment support it to use this file.

# Model fitting
## Linear regression
All linear regression learning model could be called in `implementations.py`
## Logistic regression
For logistic regression, we provide a simple example for quick hands on.
```python
from model import LogisticRegression

# Training data reading as y, tx
# Testing data reading as test_data, test_ids
# ...
# Do some manipulations

# Creation of logistic regression model
model = LogisticRegression((tx,y), regularizer="Lasso", regularizer_p=0.1)

# Model training, where you could specify hyper parameters.
weight = model.train(lr=0.1, decay=0.5, max_iters=2000, early_stop=400)
pred_label = predict_labels(weight, t_data)
create_csv_submission(test_ids, pred_label, get_dataset_dir() +
                      '/submission/removed_outlier_{}.csv'.format(title))


# Cross validation, for lambdas (for penalized terms)
# If your machine support matplotlib.pyplot, you could set plot=True
best_weights, best_lambda, (err_tr, err_te) = \
   logistic.cross_validation(4, lambdas, 'regularizer_p',
                             lr=0.1, batch_size=32,
                             max_iters=6000, early_stop=1000,
                             plot=True)

# load test.csv as test_data, test_ids
y_pred = []
for w in weights:
    _y_pred = logistic.__call__(test_data, w)
    y_pred += _y_pred
y_pred = np.average(y_pred)
y_pred[np.where(y_pred <= 0.5)] = -1
y_pred[np.where(y_pred > 0.5)] = 1
create_csv_submission(test_ids, y_pred, PATH_TO_CSV_SUBMISSION)
```




